import torch
import torch.nn as nn
import numpy as np
import torch.backends.cudnn as cudnn
import argparse
import time
import shutil
import os
import logging
import sys
import wandb
from models import split_models
import ml_utils
from tqdm import tqdm

ml_utils.enable_gpu_benchmarking()


def get_accuracy(true_labels, pred_labels):
    true_labels = true_labels.detach().cpu().numpy()
    pred_labels = pred_labels.detach().cpu().numpy()
    pred_labels_mapped = np.ones_like(pred_labels) * -1
    n_classes = true_labels.max()

    for i in reversed(range(n_classes + 1)):
        label_map = np.bincount(pred_labels[true_labels == i]).argmax()
        pred_labels_mapped[pred_labels == label_map] = i
    return (pred_labels_mapped == true_labels).astype(float).mean()


def compute_grad_loss_dl(dl, criterion_ce, criterion_grad, model, ys_logits):
    device = ml_utils.get_device()
    grad_losses = []
    for index, p_hat, grad, label in tqdm(dl, leave=False):
        batch_size = label.shape[0]
        p_hat = p_hat.to(device)
        p_hat.requires_grad = True
        grad = grad.to(device) / batch_size
        y_logits = ys_logits[index].to(device)
        y = torch.softmax(y_logits, dim=-1)
        p = model.g(p_hat)
        ce_loss = criterion_ce(p, y)
        grad_approx = torch.autograd.grad(ce_loss, p_hat, create_graph=True)[0]
        grad_loss = criterion_grad(grad, grad_approx) / (grad ** 2).mean()
        grad_losses.append(grad_loss.cpu().detach())
    grad_loss = torch.cat(grad_losses).mean()
    return grad_loss


cos = nn.CosineSimilarity(dim=-1, eps=1e-6)


def compute_grad_loss(grad_approx, grad_orig):
    grad_loss = -cos(grad_approx, grad_orig)
    return grad_loss


def main():
    """
    1. Load model
    2. Collect gradients of the loss wrt p_hat (grad) for all examples (x) in training data (where p_hat = f(x))
    3. Train a model g' such that the gradients (grad') match up with the ones collected in step 2 (grad) by optimizing over \theta_g' and y' (where y' is the guessed labels)
    """
    # torch.multiprocessing.set_sharing_strategy("file_system")
    cudnn.enabled = True
    cudnn.benchmark = True
    ml_utils.set_seed(202)
    ml_utils.enable_gpu_benchmarking()
    wandb.init(project="exploit")

    start = time.time()
    parser = argparse.ArgumentParser(description="Label leakage")
    parser.add_argument(
        "--dataset",
        type=str,
        default="mnist",
        help="dataset",
        choices=ml_utils.ds_choices,
    )
    parser.add_argument(
        "--opt_model",
        type=str,
        default="adam",
        help="optimizer",
        choices=["sgd", "adam"],
    )
    parser.add_argument(
        "--opt_y",
        type=str,
        default="adam",
        help="optimizer",
        choices=["sgd", "adam"],
    )
    parser.add_argument(
        "--loss_opt",
        type=str,
        default="grad",
        help="loss_opt mode",
        choices=["grad", "full"],
    )
    parser.add_argument("--lr_model", type=float, default=0.001, help="learning rate")
    parser.add_argument("--lr_y", type=float, default=0.001, help="learning rate")
    parser.add_argument("--lamb_grad", type=float, default=1, help="lambda grad")
    parser.add_argument("--lamb_acc", type=float, default=1, help="lambda accuracy")
    parser.add_argument("--lamb_lp", type=float, default=1, help="lambda label prior")
    parser.add_argument(
        "--weight_decay_model", type=float, default=0, help="weight decay"
    )
    parser.add_argument("--weight_decay_y", type=float, default=0, help="weight decay")
    parser.add_argument("--epochs", type=int, default=200, help="number of epochs")
    parser.add_argument(
        "--attack_epoch", type=int, default=1, help="epochs at which attack happens"
    )
    parser.add_argument(
        "--num_workers", type=int, default=1, help="number of workers for dataloader"
    )
    parser.add_argument("--batch_size", type=int, default=512, help="batch size")
    parser.add_argument(
        "--run_name", type=str, default="alpha", help="Set wandb run name"
    )
    parser.add_argument(
        "--sch_model",
        type=str,
        default="None",
        help="Scheduler for model",
        choices=["None", "cosine"],
    )
    parser.add_argument(
        "--sch_y",
        type=str,
        default="None",
        help="Scheduler for y",
        choices=["None", "cosine"],
    )
    parser.add_argument(
        "--model",
        type=str,
        default="conv3",
        help="Model",
        choices=split_models.keys(),
    )

    parser.add_argument(
        "--dry_run", action="store_true", help="disable wandb cloud sync"
    )
    parser.add_argument(
        "--gpu_id", type=str, default="0", help="GPU ID", choices=["-1", "0", "1"]
    )
    parser.add_argument(
        "--enable_pbar", action="store_true", help="enable progress bar"
    )
    parser.add_argument(
        "--multi_party", action="store_true", help="multi-party split learning"
    )

    args = parser.parse_args()
    torch.set_num_threads(args.num_workers)
    wandb.config.update(args)
    if args.gpu_id == -1:
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
    os.environ["CUDA_VISIBLE_DEVICES"] = f"{args.gpu_id}"

    if args.dry_run:
        os.environ["WANDB_MODE"] = "dryrun"

    exp_path = f"./exp/{args.dataset}/{args.run_name}/{args.attack_epoch}"

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    output_file_handler = logging.FileHandler(f"{exp_path}/attack.log")
    stdout_handler = logging.StreamHandler(sys.stdout)
    logger.addHandler(output_file_handler)
    logger.addHandler(stdout_handler)

    logger.info(f"Running ExPLoit attack on {args.dataset}")
    logger.info(f"exp_path: {exp_path}")
    logger.info(f"wandb url: {wandb.run.get_url()}\n")
    model_orig = split_models[args.model](
        x_dim=ml_utils.xdim_dict[args.dataset],
        n_classes=ml_utils.nclasses_dict[args.dataset],
        n_channels=ml_utils.nch_dict[args.dataset],
        adversary=False,
    )

    model_orig.load_state_dict(
        torch.load(f"{exp_path}/split_model.pt", map_location="cpu")
    )
    logger.info(f"Loaded model: {exp_path}/split_model.pt")

    grad_data = np.load(exp_path + "/grads.npz")
    p_hats, grads, labels = (
        torch.tensor(grad_data["p_hat"]),
        torch.tensor(grad_data["grads"]),
        torch.tensor(grad_data["ys"]),
    )
    logger.info(f"Loaded gradient data: {exp_path}/grads.npz")

    model_approx = split_models[args.model](
        x_dim=ml_utils.xdim_dict[args.dataset],
        n_classes=ml_utils.nclasses_dict[args.dataset],
        n_channels=ml_utils.nch_dict[args.dataset],
        adversary=True,
    )
    model_approx.train()

    indices = torch.arange(p_hats.shape[0]).long()
    device = ml_utils.get_device()

    ds = torch.utils.data.TensorDataset(indices, p_hats, grads, labels)

    dl = torch.utils.data.DataLoader(
        ds,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        shuffle=True,
        pin_memory=True,
    )
    ys_logits = (
        torch.zeros(grads.shape[0], ml_utils.nclasses_dict[args.dataset])
        .to(device)
        .requires_grad_(True)
    )
    model_approx = model_approx.to(device)
    opt_model = ml_utils.get_optimizer(
        args.opt_model,
        model_approx.g.parameters(),
        args.lr_model,
        args.weight_decay_model,
    )
    opt_y = ml_utils.get_optimizer(
        args.opt_y,
        [ys_logits],
        args.lr_y,
        args.weight_decay_y,
    )
    sch_model = ml_utils.get_scheduler(args.sch_model, opt_model, args.epochs)
    sch_y = ml_utils.get_scheduler(args.sch_y, opt_y, args.epochs)

    criterion_ce_soft = ml_utils.SoftCrossEntropyLoss(reduction="mean")
    criterion_kl = ml_utils.KlDivLoss(reduction="mean")
    criterion_entropy = ml_utils.EntropyLoss(logits=False)
    if args.dataset == "criteo":
        ys_prior = torch.tensor([0.8918, 1 - 0.8918], device=device).unsqueeze(0)
        ys_prior = torch.tensor([1.0, 0], device=device).unsqueeze(0)
    else:
        ys_prior = (
            torch.ones([1, ml_utils.nclasses_dict[args.dataset]], device=device)
            / ml_utils.nclasses_dict[args.dataset]
        )
    mean_norm = grads.norm(dim=-1).mean().detach().item()
    n_classes = ml_utils.nclasses_dict[args.dataset]

    if args.dataset != "criteo":
        H_y = -np.log(1 / n_classes)
    else:
        p = 0.8918
        H_y = -p * np.log(p) - (1 - p) * np.log(1 - p)

    cos = nn.CosineSimilarity(dim=-1, eps=1e-6)
    eps = 0.00001
    logger.info("Learning weights (g) and labels")
    for epoch in range(1, args.epochs):
        model_approx.train()
        start_epoch = time.time()
        (
            grad_loss_list,
            acc_loss_list,
            label_entropy_list,
            loss_list,
            loss_opt_list,
            lp_loss_list,
        ) = ([], [], [], [], [], [])
        for index, p_hat, grad, label in tqdm(
            dl, leave=False, disable=~args.enable_pbar
        ):
            label = label.to(device)
            index = index.to(device)
            p_hat = p_hat.to(device)
            batch_size = grad.shape[0]
            grad = grad.to(device)
            p_hat.requires_grad = True
            opt_model.zero_grad()
            opt_y.zero_grad()
            ys = torch.softmax(ys_logits, dim=-1)
            y = ys[index]

            p = model_approx.g(p_hat)
            H_label = criterion_entropy(y)

            ce_loss = criterion_ce_soft(p, y)
            grad_approx = torch.autograd.grad(ce_loss, p_hat, create_graph=True)[0]
            grad_loss = (
                (grad - grad_approx * batch_size).norm(dim=-1)
            ).mean() / mean_norm

            acc_loss = ce_loss / H_y

            ys_mean = ys.mean(dim=0, keepdims=True)
            lp_loss = criterion_kl(ys_prior, ys_mean)
            ys_labels = torch.argmax(ys, dim=-1).detach().cpu()
            loss = (
                args.lamb_grad * grad_loss
                + args.lamb_acc * acc_loss
                + args.lamb_lp * lp_loss
            )
            if args.loss_opt == "grad":
                loss_opt = grad_loss
            elif args.loss_opt == "full":
                loss_opt = grad_loss + acc_loss + lp_loss
            else:
                raise ValueError(f"loss_opt: {args.loss_opt}")
            loss.backward()
            opt_model.step()
            opt_y.step()

            acc = get_accuracy(labels.clone(), ys_labels.clone()) * 100

            grad_loss_list.append(grad_loss.detach().cpu().item())
            acc_loss_list.append(acc_loss.detach().cpu().item())
            lp_loss_list.append(lp_loss.detach().cpu().item())
            label_entropy_list.append(H_label.detach().cpu().item())
            loss_list.append(loss.detach().cpu().item())
            loss_opt_list.append(loss_opt.detach().cpu().item())

        if sch_model:
            sch_model.step()
        if sch_y:
            sch_y.step()

        loss_mean = np.mean(loss_list)
        loss_opt_mean = np.mean(loss_opt_list)
        grad_loss_mean = np.mean(grad_loss_list)
        acc_loss_mean = np.mean(acc_loss_list)
        lp_loss_mean = np.mean(lp_loss_list)
        label_entropy_mean = np.mean(label_entropy_list)
        end_epoch = time.time()
        time_epoch = end_epoch - start_epoch

        logger.info(
            "[Epoch: {:3}] acc: {:.2f} loss: {:.4f} loss_opt: {:.4f} grad_loss: {:.4f} acc_loss: {:.4f} label_entropy: {:.4f} lp_loss: {:.4f} time/epoch: {:.2f}".format(
                epoch,
                acc,
                loss_mean,
                loss_opt_mean,
                grad_loss_mean,
                acc_loss_mean,
                label_entropy_mean,
                lp_loss_mean,
                time_epoch,
            )
        )
        wandb.log(
            {
                "accuracy": acc,
                "loss": loss_mean,
                "loss_opt": loss_opt_mean,
                "grad_loss": grad_loss_mean,
                "acc_loss": acc_loss_mean,
                "label_entropy": label_entropy_mean,
                "lp_loss": lp_loss_mean,
            }
        )

    torch.save(model_approx.state_dict(), f"{exp_path}/split_model_approx.pt")
    logger.info(f"Stolen model saved at: {exp_path}/split_model_approx.pt")
    end = time.time()
    seconds = end - start
    hour = int(seconds / (60 * 60))
    min = int(seconds / 60) % 60
    sec = int(seconds) % 60
    logger.info("Runtime: {}:{}:{}".format(hour, min, sec))
    print("attack_accuracy: ", acc)


if __name__ == "__main__":
    main()

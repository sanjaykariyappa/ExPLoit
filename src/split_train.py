import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import argparse
import time
import shutil
import os
import logging
import sys
import wandb
from models import split_models
from tqdm import tqdm
import ml_utils
import numpy as np

torch.multiprocessing.set_sharing_strategy("file_system")
cudnn.enabled = True
cudnn.benchmark = True
ml_utils.set_seed(202)
ml_utils.enable_gpu_benchmarking()
wandb.init(project="exploit")


def grad_noise(grad, sigma, clip):
    batch_size = grad.size(0)
    grad *= batch_size
    if clip >= 0:
        grad_clip = grad / torch.max(
            torch.tensor(1.0), grad.norm(dim=-1) / clip
        ).unsqueeze(-1)
        if grad_clip.norm(dim=-1).max() > clip + 1e-5:
            print("Gradient clipping error")
            sys.exit(0)
    else:
        grad_clip = grad

    grad_clip /= batch_size
    return grad_clip + torch.clamp(
        torch.randn_like(grad) * sigma, -3 * sigma, 3 * sigma
    )


def main():
    """
    1. Train a split model: p_hat = f(x); p = g(y_hat)
    2. Save model
    """
    start = time.time()
    parser = argparse.ArgumentParser(description="Label leakage")
    parser.add_argument(
        "--dataset",
        type=str,
        default="mnist",
        help="dataset",
        choices=ml_utils.ds_choices,
    )
    parser.add_argument(
        "--opt",
        type=str,
        default="adam",
        help="optimizer",
        choices=["sgd", "adam"],
    )
    parser.add_argument("--lr", type=float, default=1e-3, help="learning rate")
    parser.add_argument("--weight_decay", type=float, default="0.", help="weight decay")
    parser.add_argument("--epochs", type=int, default=10, help="number of epochs")
    parser.add_argument("--batch_size", type=int, default=512, help="batch size")
    parser.add_argument(
        "--run_name", type=str, default="alpha", help="Set wandb run name"
    )
    parser.add_argument("--grad_clip", type=float, default=-1, help="Gradient Clip")
    parser.add_argument(
        "--grad_sigma", type=float, default=0, help="Gradient noise sigma"
    )

    parser.add_argument(
        "--sch",
        type=str,
        default="None",
        help="Scheduler",
        choices=["None", "cosine"],
    )
    parser.add_argument(
        "--model",
        type=str,
        default="conv3",
        help="Model",
        choices=split_models.keys(),
    )
    parser.add_argument(
        "--dry_run", action="store_true", help="disable wandb cloud sync"
    )
    parser.add_argument(
        "--enable_pbar", action="store_true", help="enable progress bar"
    )
    parser.add_argument(
        "--multi_party", action="store_true", help="multi-party split learning"
    )
    parser.add_argument(
        "--save_grad_last_only",
        action="store_true",
        help="only save gradients for last epoch",
    )

    parser.add_argument(
        "--gpu_id", type=str, default="0", help="GPU ID", choices=["-1", "0", "1"]
    )
    parser.add_argument(
        "--num_workers", type=int, default=8, help="number of workers for dataloader"
    )

    args = parser.parse_args()
    torch.set_num_threads(args.num_workers)
    wandb.config.update(args)
    if args.gpu_id == -1:
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
    os.environ["CUDA_VISIBLE_DEVICES"] = f"{args.gpu_id}"

    if args.dry_run:
        os.environ["WANDB_MODE"] = "dryrun"

    exp_path = f"./exp/{args.dataset}/{args.run_name}/"
    if os.path.exists(exp_path):
        shutil.rmtree(exp_path)
    os.makedirs(exp_path)

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    output_file_handler = logging.FileHandler(f"{exp_path}/train.log")
    stdout_handler = logging.StreamHandler(sys.stdout)
    logger.addHandler(output_file_handler)
    logger.addHandler(stdout_handler)

    logger.info(f"Training split model on {args.dataset}")
    logger.info(f"exp_path: {exp_path}")
    logger.info(f"wandb url: {wandb.run.get_url()}\n")
    train_loader, test_loader = ml_utils.get_dataloaders(
        args.dataset, args.batch_size, augment=True, num_workers=args.num_workers
    )
    n_classes = ml_utils.nclasses_dict[args.dataset]

    model = split_models[args.model](
        x_dim=ml_utils.xdim_dict[args.dataset],
        n_classes=n_classes,
        n_channels=ml_utils.nch_dict[args.dataset],
    )

    criterion = nn.CrossEntropyLoss(reduction="mean")
    opt = ml_utils.get_optimizer(
        args.opt, model.parameters(), args.lr, args.weight_decay
    )
    sch = ml_utils.get_scheduler(args.sch, opt, args.epochs)
    device = ml_utils.get_device()
    model = model.to(device)
    grads = []
    p_hats = []
    ys = []
    test_acc_list = []
    if args.dataset != "criteo":
        H_y = -np.log(1 / n_classes)
    else:
        p = 0.8918
        H_y = -p * np.log(p) - (1 - p) * np.log(1 - p)

    x_dim_half = int(ml_utils.xdim_dict[args.dataset] / 2)

    for epoch in range(1, args.epochs + 1):
        s = time.time()
        train_loss = correct = 0
        model.train()
        for (x, y) in tqdm(
            train_loader, ncols=80, disable=~args.enable_pbar, leave=False
        ):
            # Forward pass
            x, y = x.to(device), y.to(device)
            batch_size = x.shape[0]
            opt.zero_grad()
            if args.multi_party:
                """
                p_hat_a = model.f_a(x[:, :, :x_dim_half, :x_dim_half])
                p_hat_b = model.f_b(x[:, :, x_dim_half:, :x_dim_half])
                p_hat_c = model.f_c(x[:, :, :x_dim_half, x_dim_half:])
                p_hat_d = model.f_d(x[:, :, x_dim_half:, x_dim_half:])
                p_hat = torch.cat([p_hat_a, p_hat_b, p_hat_c, p_hat_d], dim=-1)
                """

                p_hat_a = model.f_a(x[:, :, :, :x_dim_half])
                p_hat_b = model.f_b(x[:, :, :, x_dim_half:])
                p_hat = torch.cat([p_hat_a, p_hat_b], dim=-1)
            else:
                p_hat = model.f(x)
            p_hat.retain_grad()
            p = model.g(p_hat)
            loss = criterion(p, y)

            # Backward pass
            loss.backward(retain_graph=True)

            # Clip and add noise to gradient
            if args.grad_sigma > 0:
                p_hat_grad_mod = grad_noise(
                    p_hat.grad.detach(), args.grad_sigma, args.grad_clip
                )

                # Backpropagate through f with modified gradient
                if args.multi_party:
                    model.f_a.zero_grad()
                    model.f_b.zero_grad()
                    model.f_c.zero_grad()
                    model.f_d.zero_grad()
                else:
                    model.f.zero_grad()
                p_hat.backward(p_hat_grad_mod)
            else:
                p_hat_grad_mod = p_hat.grad.detach()

            # Log embedding, gradient and labels
            if args.multi_party:
                p_hats.append(p_hat_a.detach().cpu())
                grads.append(
                    p_hat_grad_mod.detach().cpu()[:, : int(p_hat_a.shape[-1])]
                    * batch_size
                )
            else:
                p_hats.append(p_hat.detach().cpu())
                grads.append(p_hat_grad_mod.detach().cpu() * batch_size)
            ys.append(y.cpu())

            opt.step()
            train_loss += loss.item()

            if type(p) is tuple:
                p = p[0]
            pred_class = torch.argmax(p, dim=-1)
            if y.ndim == 2:
                y = torch.argmax(y, dim=-1)
            correct += (pred_class == y).sum().item()

        train_loss /= len(train_loader)
        train_acc = correct / len(train_loader.dataset)
        test_acc, test_ce = ml_utils.test_acc_ce(model, test_loader)
        test_nce = test_ce / H_y
        train_nce = train_loss / H_y
        if sch:
            sch.step()
        e = time.time()
        time_epoch = e - s
        wandb.log(
            {
                "loss": train_loss,
                "train_acc": train_acc * 100,
                "test_acc": test_acc * 100,
                "test_nce": test_nce,
            }
        )

        print_stmt = "Epoch: {} train_loss: {:.3f} train_acc: {:.2f}%, test_acc: {:.2f}%, train_nce: {:.3f}, test_nce: {:.3f} time: {:.1f}".format(
            epoch,
            train_loss,
            train_acc * 100,
            test_acc * 100,
            train_nce,
            test_nce,
            time_epoch,
        )
        test_acc_list.append(test_acc * 100)
        if logger:
            logger.info(print_stmt)
        else:
            print(print_stmt)

        grads = torch.cat(grads).numpy()
        p_hats = torch.cat(p_hats).numpy()
        ys = torch.cat(ys).numpy()
        save_path = f"{exp_path}/{epoch}"
        os.makedirs(save_path)
        if (
            args.save_grad_last_only and epoch == args.epochs
        ) or not args.save_grad_last_only:
            np.savez(f"{save_path}/grads.npz", p_hat=p_hats, ys=ys, grads=grads)
            torch.save(model.state_dict(), f"{save_path}/split_model.pt")
        grads = []
        p_hats = []
        ys = []

    end = time.time()
    seconds = end - start
    hour = int(seconds / (60 * 60))
    min = int(seconds / 60) % 60
    sec = int(seconds) % 60
    logger.info("Runtime: {}:{}:{}".format(hour, min, sec))
    print("test_acc:\n")
    for t in test_acc_list:
        print(t)


if __name__ == "__main__":
    main()
